---
title: Kaggle Multimodal Single-Cell Integration æŒ¯ã‚Šè¿”ã‚Š
slug: kaggle-multimodal-single-cell-integration
date: 2022-11-24
updated:
published: true
tags:
    - Compete
    - Kaggle
    - Machine Learning
    - Deep Learning
series: false
cover_image: ./images/195366837-9048d24c-86ca-42d6-99a8-414a019a5048.png
canonical_url: false
description: "Multimodal Single-Cell Integration ã‚³ãƒ³ãƒšã®æŒ¯ã‚Šè¿”ã‚Šã§ã™ã€‚"
---

ã‚³ãƒ³ãƒšã§å–ã‚Šçµ„ã‚“ã ã“ã¨ã‚’ä¸­å¿ƒã«æŒ¯ã‚Šè¿”ã‚Šã§ã™ã€‚

ã“ã®ã‚³ãƒ³ãƒšã¯ã€Private ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«ã¯å­˜åœ¨ã—ãªã„å°†æ¥ã®æ—¥ä»˜ã®ã‚‚ã®ã§ã€
ã„ã‚ã‚†ã‚‹ãƒ‰ãƒ¡ã‚¤ãƒ³æ±åŒ–æ€§èƒ½ãŒå•ã‚ã‚Œã‚‹ã‚³ãƒ³ãƒšã§ã—ãŸã€‚
ä¸€æ–¹ã§ã€æ—¥ä»˜ã«ã‚ˆã‚‹å¤‰åŒ–ã®è¦ç´ ã‚‚ã‚ã‚Šã€å®Œå…¨ã«æ—¥ä»˜ã®ç‰¹å¾´ã‚’ç„¡è¦–ã™ã‚‹ã®ã‚‚æœ›ã¾ã—ããªã„ã“ã¨ãŒã€
äºˆæƒ³ã•ã‚Œã¦ã„ã¾ã—ãŸã€‚

ã¯ã˜ã‚ã«ã€Adversarial training (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯) ã‚’è¡Œã£ãŸã¨ã“ã‚ã€
Citeseq ã¯ 99% åˆ†é¡ãŒå¯èƒ½ãªçŠ¶æ³ã§ã€ã“ã®ç‰¹å¾´é‡ã§å­¦ç¿’ã‚’ã™ã‚‹ã¨ã€å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¸ã®éå­¦ç¿’ãŒ
æ‡¸å¿µã•ã‚Œã‚‹ã¨è€ƒãˆã‚‰ã‚Œã¾ã—ãŸã€‚
ã—ã‹ã—ã€Adversarial training ã®ç²¾åº¦ã‚’ä¸‹ã’ã‚‹ãŸã‚ã«ã€ç‰¹å¾´é‡ã‚’æ¸›ã‚‰ã™ã¨ã€è‘—ã—ãã€Public LBã®ã‚¹ã‚³ã‚¢ã‚‚
ä¸‹ãŒã‚‹çŠ¶æ³ã§ã€åŸºæœ¬çš„ã«ã¯ã€ç‰¹å¾´é‡ã‚’æ¸›ã‚‰ã™ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯å³ã—ã„ã®ã§ã¯ãªã„ã‹ã¨è€ƒãˆã¾ã—ãŸã€‚

ãã“ã§ã€ãªã«ã‹ç”Ÿç‰©å­¦çš„ãªç‰¹å¾´é‡ã®å·¥å¤«ã‚’è¡Œã†ã“ã¨ã‚„ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹
æ±åŒ–æ€§èƒ½ã®å‘ä¸Šã‚’ç›®æŒ‡ã™æ–¹é‡ã§è‡¨ã¿ã¾ã—ãŸã€‚

---

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [âœ¨ Result](#-result)
- [ğŸ–¼ï¸ Solution](#-solution)
  - [ğŸŒ± Preprocess](#-preprocess)
  - [ğŸ¤¸ Pre Training](#-pre-training)
  - [ğŸƒ Training](#-training)
  - [ğŸ¨ Base Models](#-base-models)
  - [ğŸš€ Postprocess](#-postprocess)
- [ğŸ’¡ Tips](#-tips)
  - [Pearson Loss for XGBoost](#pearson-loss-for-xgboost)
- [ğŸ·ï¸ Links](#-links)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

---


## âœ¨ Result

- Private: 0.769808
- Public: 0.813093



## ğŸ–¼ï¸ Solution


### ğŸŒ± Preprocess

- Citeseq
    - å…ƒãƒ‡ãƒ¼ã‚¿ ã‚’ PCA ã§ 100æ¬¡å…ƒã«å‰Šæ¸›ã—ã¾ã—ãŸã€‚
    - ä¸€æ–¹ã§ã€é‡è¦ã‚«ãƒ©ãƒ ã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¸©å­˜ã—ã¾ã—ãŸã€‚
    - [ivis](https://bering-ivis.readthedocs.io/en/latest/index.html) ã®æ•™å¸«ãªã—å­¦ç¿’ã§ 100æ¬¡å…ƒã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚
    - åˆã‚ã›ã¦ã€ãƒŸãƒˆã‚³ãƒ³ãƒ‰ãƒªã‚¢ã®RNAç´°èƒã”ã¨ã®å’Œã‚’ç‰¹å¾´é‡ã«è¿½åŠ ã—ã¾ã—ãŸã€‚
    - Metadata ã® Cell type ã‚’ç‰¹å¾´é‡ã«è¿½åŠ ã—ã¾ã—ãŸã€‚
- Multiome
    - åˆ—åã®æ¥é ­æ–‡å­—ãŒåŒã˜ã‚°ãƒ«ãƒ¼ãƒ—ã”ã¨ã« PCA ã§ ãŠã‚ˆã å„ 100 æ¬¡å…ƒã«å‰Šæ¸›ã—ã¾ã—ãŸã€‚
    - ã•ã‚‰ã« ivis ã®æ•™å¸«ãªã—å­¦ç¿’ã§ 100 æ¬¡å…ƒã®ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚


### ğŸ¤¸ Pre Training

- Adversarial training (å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’åˆ†é¡ã™ã‚‹ã‚¿ã‚¹ã‚¯) ã‚’è¡Œã„ã€èª¤åˆ¤å®šã•ã‚ŒãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’ good validation ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¾ã™ã€‚
- Multiome ã® Cell type ã®äºˆæ¸¬ã‚’è¡Œã„ã€ç‰¹å¾´é‡ã«è¿½åŠ ã—ã¾ã—ãŸã€‚


### ğŸƒ Training

- good validation ãƒ‡ãƒ¼ã‚¿ã‚’ æ­£ãƒ©ãƒ™ãƒ« ã¨ã—ãŸ StratifiedKFold ã§ã™ã€‚
- Loss é–¢æ•°ã« Pearson ç›¸é–¢ä¿‚æ•° ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ XGBoost ã¯å¾Œè¿°ã®æ–¹æ³•ã§å®Ÿè£…ã—ã¾ã—ãŸã€‚
- TabNet ã¯ Pre-training ã‚‚è¡Œã„ã¾ã—ãŸã€‚ (ã“ã®ã‚³ãƒ³ãƒšã§ã¯ã€Pre-training ã‚’è¡Œã£ãŸã»ã†ãŒç²¾åº¦ãŒè‰¯ã‹ã£ãŸã§ã™)


### ğŸ¨ Base Models

- Citeseq
    - TabNet
    - Simple MLP
    - ResNet
    - 1D CNN
    - XGBoost
- Multiome
    - 1D CNN

Citeseq ã¯ã•ã¾ã–ã¾ãªãƒ¢ãƒ‡ãƒ«ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã‚¹ã‚³ã‚¢ãŒã®ã³ã¾ã—ãŸã€‚
ä¸€æ–¹ã€ Multiome ã¯ 1D CNN ãŒå¼·ãã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã¦ã‚‚ã‚¹ã‚³ã‚¢ãŒä¼¸ã³ãšã€1D CNN ã®ã¿ä½¿ç”¨ã—ã¾ã—ãŸã€‚

### ğŸš€ Postprocess

- è©•ä¾¡æŒ‡æ¨™ãŒ Pearson ç›¸é–¢ä¿‚æ•°ãªã®ã§ã€ å„æ¨è«–çµæœ(OOFçµæœå«ã‚)ã‚’ æ­£è¦åŒ– ã—ã¦ã‹ã‚‰ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã¾ã—ãŸã€‚
- Optuna ã§ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã®é‡ã¿ã‚’æœ€é©åŒ–ã—ã¾ã—ãŸã€‚è©•ä¾¡æŒ‡æ¨™ã«ã¯ good validation ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚
- Public Notebook x2 ã¨ ãƒãƒ¼ãƒ ãƒ¡ã‚¤ãƒˆ ã® ã‚µãƒ–ãƒŸãƒƒã‚·ãƒ§ãƒ³ ã¨ã‚‚ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã¾ã—ãŸã€‚


## ğŸ’¡ Tips


### Pearson Loss for XGBoost

XGBoost ã¯ Pearson Loss Function ãŒæä¾›ã•ã‚Œã¦ã„ãªã„ã®ã§ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å®Ÿè£…ã—ã¾ã—ãŸã€‚
ãŸã ã—ã€ã“ã®å®Ÿè£…ã¯å­¦ç¿’ãŒé…ãã€ã‚‚ã†å°‘ã—ã€æ”¹å–„ã‚’ã—ãŸã„å°è±¡ã‚’æŒã£ã¦ã„ã¾ã™ã€‚ã€‚

```
from functools import partial
from typing import Any, Callable

import numpy as np
import torch
import torch.nn.functional as F
import xgboost as xgb


def pearson_cc_loss(inputs, targets):
    try:
        assert inputs.shape == targets.shape
    except AssertionError:
        inputs = inputs.view(targets.shape)

    pcc = F.cosine_similarity(inputs, targets)
    return 1.0 - pcc


# https://towardsdatascience.com/jax-vs-pytorch-automatic-differentiation-for-xgboost-10222e1404ec
def torch_autodiff_grad_hess(
    loss_function: Callable[[torch.Tensor, torch.Tensor], torch.Tensor], y_true: np.ndarray, y_pred: np.ndarray
):
    """
    Perform automatic differentiation to get the
    Gradient and the Hessian of `loss_function`.
    """
    y_true = torch.tensor(y_true, dtype=torch.float, requires_grad=False)
    y_pred = torch.tensor(y_pred, dtype=torch.float, requires_grad=True)
    loss_function_sum = lambda y_pred: loss_function(y_true, y_pred).sum()

    loss_function_sum(y_pred).backward()
    grad = y_pred.grad.reshape(-1)

    # hess_matrix = torch.autograd.functional.hessian(loss_function_sum, y_pred, vectorize=True)
    # hess = torch.diagonal(hess_matrix)
    hess = np.ones(grad.shape)

    return grad, hess


custom_objective = partial(torch_autodiff_grad_hess, pearson_cc_loss)


xgb_params = dict(
    n_estimators=10000,
    early_stopping_rounds=20,
    # learning_rate=0.05,
    objective=custom_objective,  # "binary:logistic", "reg:squarederror",
    eval_metric=pearson_cc_xgb_score,  # "logloss", "rmse",
    random_state=440,
    tree_method="gpu_hist",
)  # type: dict[str, Any]

clf = xgb.XGBRegressor(**xgb_params)
```


## ğŸ·ï¸ Links

- [My Solution](https://github.com/IMOKURI/kaggle-multimodal-single-cell-integration)

